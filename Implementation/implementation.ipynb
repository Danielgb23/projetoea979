{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection, tracking and location of runways on visual approaches\n",
    "\n",
    "projeto realizado para o curso \"EA979 - INTRODUÇÃO À COMPUTAÇÃO GRÁFICA E AO PROCESSAMENTO DE IMAGEM\"\n",
    "\n",
    "- Daniel Gonçalves Benvenutti 169448\n",
    "- Matteo Di Fabio 264339\n",
    "- Renan Sterle 176536\n",
    "\n",
    "## Introduction and Motivation\n",
    "\n",
    "Both in military aviation as well as in civil aviation, visual flight rules (VFR) remain of great importance nowadays. Particularly in airports and terminals of reduced infrastructure, visual approach and landing procedures account for the majority of cases and such procedures rely on visibility conditions, visual navigational aids and personal performance of the pilot in command. \n",
    "In any type of landing approach, the maintenance of a safe flight envelope is crucial. Therefore, during the final approach leg, parameters such as airspeed, altitude, descent rate, glide slope, ground clearance, as well as location in relation to the runway must be monitored and corrected.\n",
    "While airspeed, altitude and descent rate are measure by the aircraft, the other parameters can only be determined with the use of external information. In the case of instrument flight rules (IFR), Instrument Landing Systems (ILS), Global Navigation Satellite System (GNSS), inertial navigation systems and radio beacons provide such information, whereas on VFR they are ultimately estimated visually\n",
    "by the pilot.\n",
    "\n",
    "The idea of this project is to implement a system that:\n",
    "\n",
    "1. Given a picture of an approach, identifies and isolates the runway in question, using image processing filtering and segmentation techniques (parágrafo IdentifyRunway);\n",
    "2. Fits the runway image into the expected runway shape by applying perspective and affine transformations (parágrafo Correct);\n",
    "3. Determines the camera point of view in relation to the runway start. Alignment, lateral offset and distance information are the most important, while relative altitude is of secondary importance (parágrafo Calculos openCV);\n",
    "4. Tracks the identified runway in a stream of frames, validating and checking the correlation of the results between different frames of a temporal sequence; (essa etapa è da remover????)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the list to see if i \"linked\" the right paragraph/code\n",
    "# Daniel code is missing i don't know where to put it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imported library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import numpy as NP;\n",
    "from PIL import Image;\n",
    "import matplotlib;\n",
    "#matplotlib.use(\"GTK3Agg\");\n",
    "import matplotlib.pyplot as PLT;\n",
    "import json;\n",
    "\n",
    "\n",
    "import cv2;\n",
    "import numpy as np;\n",
    "import operator;\n",
    "import copy;\n",
    "\n",
    "#import os.path;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct.py explanation\n",
    "    TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PerspectiveCoefficients(A, B):\n",
    "    matrix = [];\n",
    "    for p1, p2 in zip(A, B):\n",
    "        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]]);\n",
    "        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]]);\n",
    "\n",
    "    A = NP.matrix(matrix, dtype=NP.float)\n",
    "    B = NP.array(B).reshape(8)\n",
    "\n",
    "    res = NP.dot(NP.linalg.inv(A.T * A) * A.T, B)\n",
    "    return NP.array(res).reshape(8)\n",
    "\n",
    "def Correction(I, Points, Width, Length):\n",
    "    L0 = NP.array(Points[\"L0\"]);\n",
    "    R0 = NP.array(Points[\"R0\"]);\n",
    "    L1 = NP.array(Points[\"L1\"]);\n",
    "    R1 = NP.array(Points[\"R1\"]);\n",
    "\n",
    "    C0 = (L0+R0)/2.0;\n",
    "    C1 = (L1+R1)/2.0;\n",
    "    C = (C0+C1)/2.0;\n",
    "    D = C0-C1;\n",
    "    Angle = NP.arctan2(D[0], D[1]);\n",
    "\n",
    "    #Rotate image:\n",
    "    I = I.rotate(NP.degrees(-Angle), Image.BICUBIC, False, (C[0], C[1]));\n",
    "\n",
    "    #Rotate points:\n",
    "    Cos, Sin = NP.cos(Angle), NP.sin(Angle);\n",
    "    Rotation = NP.array(((Cos, -Sin), (Sin, Cos)));\n",
    "    [L0, R0, L1, R1, C_] = NP.transpose(NP.matmul(Rotation, NP.c_[L0, R0, L1, R1, C]));\n",
    "\n",
    "    #Correct center offset:\n",
    "    D = C-C_;\n",
    "    [L0, R0, L1, R1] = [L0, R0, L1, R1]+D;\n",
    "\n",
    "    D = R0-L0;\n",
    "    ShearFactor = D[1]/D[0];\n",
    "    #Shear image:\n",
    "    Shear = NP.eye(3);\n",
    "    Shear[1][0] = ShearFactor;\n",
    "    I = I.transform(I.size, Image.AFFINE, Shear.flatten()[:6], Image.BICUBIC);\n",
    "\n",
    "    #Shear points:\n",
    "    Shear = NP.eye(2);\n",
    "    Shear[1][0] = -ShearFactor;\n",
    "    [L0, R0, L1, R1] = NP.transpose(NP.matmul(Shear, NP.c_[L0, R0, L1, R1]));\n",
    "\n",
    "    #Crop image:\n",
    "    Y0 = (R0[1]+L0[1])/2.0;\n",
    "    Y1 = (R1[1]+L1[1])/2.0;\n",
    "    X0 = L0[0];\n",
    "    X1 = X0+R0[0]-L0[0];\n",
    "    I = I.crop((X0, Y1, X1, Y0));\n",
    "\n",
    "    D = NP.array([X0, Y1]);\n",
    "    [L0, R0, L1, R1] = [L0, R0, L1, R1]-D;\n",
    "\n",
    "    #Perspective:\n",
    "    W, H = I.size;\n",
    "    T = PerspectiveCoefficients([(0, H), (W, H), (0, 0), (W, 0)], [L0, R0, L1, R1]);\n",
    "    I = I.transform(I.size, Image.PERSPECTIVE, T.flatten()[:8], Image.BICUBIC);\n",
    "    [L0, R0, L1, R1] = [(0, H), (W, H), (0, 0), (W, 0)];\n",
    "\n",
    "    #Resize:\n",
    "    AspectRatioFactor = 0.4;\n",
    "    VScale = AspectRatioFactor*Length/Width;\n",
    "    I = I.resize((I.size[0], int(VScale*I.size[1])), Image.ANTIALIAS)\n",
    "    [L0, R0, L1, R1] = [(0, VScale*H), (W, VScale*H), (0, 0), (W, 0)];\n",
    "    \n",
    "    return I, NP.transpose(NP.vstack([L0, L1, R1, R0, L0]));\n",
    "\n",
    "def Correct(JSON, ExportPath):\n",
    "    #Opens original image:\n",
    "    I = Image.open(JSON[\"Image\"]);\n",
    "\n",
    "    Runway = JSON[\"Runway\"];\n",
    "    Width = NP.array(Runway[\"Width\"]);\n",
    "    Length = NP.array(Runway[\"Length\"]);\n",
    "\n",
    "    A, PA = Correction(I, JSON[\"Runway\"], Width, Length);\n",
    "    B, PB = Correction(I, JSON[\"Results\"], Width, Length);\n",
    "\n",
    "    Figure = PLT.figure(\"Correction\")\n",
    "    F = Figure.add_subplot(1, 2, 1);\n",
    "    F.imshow(A);\n",
    "    F.plot(PA[0], PA[1]);\n",
    "    F.title.set_text(\"Expected:\");\n",
    "    F = Figure.add_subplot(1, 2, 2);\n",
    "    F.imshow(B);\n",
    "    F.plot(PB[0], PB[1]);\n",
    "    F.title.set_text(\"Obtained:\");\n",
    "    \n",
    "    #Export plot:\n",
    "    if (ExportPath is not None):\n",
    "        Figure.savefig(ExportPath + \"/10 Correct.png\");\n",
    "\n",
    "    PLT.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display.py explanation\n",
    "\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Display(JSON, ExportPath):\n",
    "    Figure = PLT.figure(\"Identification\")\n",
    "    F = Figure.add_subplot(1, 1, 1);\n",
    "    F.imshow(Image.open(JSON[\"Image\"]));\n",
    "\n",
    "    Runway = JSON[\"Runway\"];\n",
    "    L0 = Runway[\"L0\"];\n",
    "    R0 = Runway[\"R0\"];\n",
    "    L1 = Runway[\"L1\"];\n",
    "    R1 = Runway[\"R1\"];\n",
    "\n",
    "    P = NP.transpose(NP.vstack([L0, L1, R1, R0, L0]));\n",
    "\n",
    "    #Plot expected:\n",
    "    F.plot(P[0], P[1], label = \"Expected boundary\", color = \"lime\");\n",
    "    F.legend();\n",
    "\n",
    "    Results = JSON[\"Results\"];\n",
    "    L0 = Results[\"L0\"];\n",
    "    R0 = Results[\"R0\"];\n",
    "    L1 = Results[\"L1\"];\n",
    "    R1 = Results[\"R1\"];\n",
    "\n",
    "    P = NP.transpose(NP.vstack([L0, L1, R1, R0, L0]));\n",
    "\n",
    "    #Plot obtained:\n",
    "    F.plot(P[0], P[1], label = \"Obtained boundary\", color = \"magenta\");\n",
    "    F.legend();\n",
    "\n",
    "    PLT.title(\"Identification Results:\")\n",
    "    PLT.show();\n",
    "    \n",
    "    #Export plot:\n",
    "    if (ExportPath is not None):\n",
    "        Figure.savefig(ExportPath + \"/09 Display.png\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(JSON, image_path) :\n",
    "    #gets image size\n",
    "    image = PLT.Image.open(image_path)\n",
    "    imageSize=image.size\n",
    "\n",
    "    #runway image points\n",
    "    L0= JSON['Results']['L0']\n",
    "    R0= JSON['Results']['R0']\n",
    "    L1= JSON['Results']['L1']\n",
    "    R1= JSON['Results']['R1']\n",
    "    imagePts=[R1,R0,L0,L1]\n",
    "    \n",
    "     #runway real world coordinates\n",
    "    #dimensoes da pista\n",
    "    Wid=Runway[\"Width\"]\n",
    "    Len=Runway[\"Length\"]\n",
    "    #real world coordinates of the edges of the runway\n",
    "    posA=[Wid,Len,0]\n",
    "    posB= [Wid,0,0]\n",
    "    posC= [0,0,0]\n",
    "    posD= [0,Len,0]\n",
    "\n",
    "    runwayPoints=[posA,posB,posC,posD]\n",
    "    #calibracao com https://www.learnopencv.com/camera-calibration-using-opencv/\n",
    "    import cv2\n",
    "    objectsPoints=np.array([runwayPoints], dtype='float32')\n",
    "    imagesPoints=np.array([imagePts],dtype='float32')\n",
    "    #finds the tranlation and rotation matrix for image points and real world coordinates\n",
    "    retval, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.calibrateCamera(objectsPoints, imagesPoints, imageSize, None, None)\n",
    "    rvecs=rvecs[0]\n",
    "    tvecs=tvecs[0]\n",
    "    \n",
    "    #rotation matrix from vector\n",
    "    R_mtx, jac = cv2.Rodrigues(rvecs)\n",
    "    #calculates the position of (0,0,0) which is the camera\n",
    "    #in real world coordinates\n",
    "    cameraPosition = -R_mtx.T * np.matrix(tvecs)\n",
    "    #np.array(cameraPosition).T[0]\n",
    "    JSON['Results']['Plane_pos']=cameraPosition\n",
    "    return JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdentifyRunway Explanation\n",
    "\n",
    "para prosseguir com a segmentação da pista de pouso, várias estradas eram praticáveis.\n",
    "uma era a conversão de imagem em escala de cinza e, em seguida, a segmentação com otsu após o pré-processamento, mas nesse caso, a grama nas laterais da pista e a pista em si tinham um cinza semelhante, às vezes até o céu, por isso era difícil encontrar valores automáticos que permitissem a segmentação. por esse motivo, tentamos segmentar uma imagem convertida em HSV, mas também nesse caso, não foi possível encontrar valores padrão que permitissem identificar a faixa em várias imagens diferentes. Nesse ponto, foi decidido agir diretamente na imagem original seguindo as seguintes etapas:\n",
    "\n",
    "1. Contraste -> realça a pista;\n",
    "2. Blur (filtragem bilateral) ->torna homogêneas as áreas com cor similares mantendo as bordas nítidas;\n",
    "3. Canny -> acha as bordas;\n",
    "4. Crop -> corta uma área de cada borda da imagem porque para algumas imagens o canny acha o contorno da imagem e isso atrapalha;\n",
    "5. Dilatação -> funde as bordas encontradas pelo canny e simplifica o processo; Com menos bordas mais grossas a chance de encontras as corretas aumenta.\n",
    "6. Hough -> identifica as retas;\n",
    "7. Filtrar as linhas -> A gente ignora as retas que são muito pouco inclinadas;\n",
    "8. Todas as retas encontradas pelo Hough são desenhadas na imagem -> os segmentos se fundam e se agrupem. (É a mesma idéia da dilatação);\n",
    "9. Hough (de novo) -> encontra as retas que são na verdade o agrupamento e simplificação das retas encontradas pelo primeiro Hough;\n",
    "10. Otimização -> de todas as retas encontradas no 9, encontramos os 4 pontos com quais podemos construir duas retas que melhor sobrepõe as bordas sobrepostas do 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cart2Pol(P):\n",
    "    Rho = np.sqrt(P[0]**2 + P[1]**2);\n",
    "    Phi = np.arctan2(P[1], P[0]);\n",
    "    return ([Rho, Phi]);\n",
    "\n",
    "def Pol2Cart(P):\n",
    "    return([P[0] * np.cos(P[1]), P[0] * np.sin(P[1])]);\n",
    "\n",
    "def IdentifyRunway(JSON, ExportPath):\n",
    "    RawImage = cv2.imread(JSON[\"Image\"]);\n",
    "    Result = copy.copy(RawImage);\n",
    "\n",
    "    # 1. Enhance contrast;\n",
    "    Alpha = 3.0;\n",
    "    Beta = -Alpha*np.mean(cv2.mean(RawImage)[0:3]);\n",
    "    Contrast = cv2.convertScaleAbs(RawImage, alpha = Alpha, beta = Beta);\n",
    "\n",
    "    # 2. Blur;\n",
    "    sigmaColor = [40, 200];\n",
    "    sigmaSpace = 10;\n",
    "    Blur1 = cv2.bilateralFilter(Contrast, d = 0, sigmaColor = sigmaColor[0], sigmaSpace = sigmaSpace, borderType = cv2.BORDER_WRAP);\n",
    "    Blur2 = cv2.bilateralFilter(Contrast, d = 0, sigmaColor = sigmaColor[1], sigmaSpace = sigmaSpace, borderType = cv2.BORDER_WRAP);\n",
    "\n",
    "    # 3. Canny;\n",
    "    Edges1 = cv2.Canny(Blur1, 100, 300);\n",
    "    Edges2 = cv2.Canny(Blur2, 80, 150);\n",
    "\n",
    "    # 4. Crop:\n",
    "    Margin = 5;\n",
    "    Edges1 = Edges1[Margin:-Margin, Margin:-Margin];\n",
    "    Edges2 = Edges2[Margin:-Margin, Margin:-Margin];\n",
    "\n",
    "    # 5. Dilatation:\n",
    "    KernelSize = 3;\n",
    "    Kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (KernelSize, KernelSize));\n",
    "    Dilation1 = cv2.dilate(Edges1, Kernel, iterations = 1);\n",
    "    Dilation2 = cv2.dilate(Edges2, Kernel, iterations = 1);\n",
    "\n",
    "    # 6. Hough 1;\n",
    "    Lines1 = cv2.HoughLinesP(Dilation1, 1, np.pi/180, threshold = 40, minLineLength = 60, maxLineGap = 5);\n",
    "    Lines2 = cv2.HoughLinesP(Dilation2, 1, np.pi/180, threshold = 40, minLineLength = 60, maxLineGap = 5);\n",
    "\n",
    "    # 7. Merge and filter lines;\n",
    "    MinAngle = 30*np.pi/180;\n",
    "    Lines = list();\n",
    "    for X in Lines1:\n",
    "        Delta = np.asarray([X[0][0], X[0][1]])-np.asarray([X[0][2], X[0][3]]);\n",
    "        Angle = np.arctan(Delta[1]/Delta[0]);\n",
    "        if (np.abs(Angle) > MinAngle):\n",
    "            Lines.append(X[0]);\n",
    "            \n",
    "    for X in Lines2:\n",
    "        Delta = np.asarray([X[0][0], X[0][1]])-np.asarray([X[0][2], X[0][3]]);\n",
    "        Angle = np.arctan(Delta[1]/Delta[0]);\n",
    "        if (np.abs(Angle) > MinAngle):\n",
    "            Lines.append(X[0]);\n",
    "\n",
    "    # 8. Line processing:\n",
    "    Edges = np.zeros(Edges1.shape, np.uint8);\n",
    "    LineWidth = 4;\n",
    "    for L in Lines:\n",
    "        cv2.line(Result, (L[0]+Margin, L[1]+Margin), (L[2]+Margin, L[3]+Margin), (0, 255, 0), 1);\n",
    "        cv2.line(Edges, (L[0]+Margin, L[1]+Margin), (L[2]+Margin, L[3]+Margin), (255, 255, 255), LineWidth);\n",
    "        cv2.circle(Result, (L[0]+Margin, L[1]+Margin), 2, (255, 0, 0), -1);\n",
    "        cv2.circle(Result, (L[2]+Margin, L[3]+Margin), 2, (255, 0, 0), -1);\n",
    "\n",
    "    # 9. Hough 2:\n",
    "    Lines = cv2.HoughLinesP(Edges, 1, np.pi/180, threshold = 20, minLineLength = 40, maxLineGap = 8);\n",
    "    DrawnEdges = cv2.cvtColor(Edges, cv2.COLOR_GRAY2RGB);\n",
    "    for X in Lines:\n",
    "        cv2.line(DrawnEdges, (X[0][0]+Margin, X[0][1]+Margin), (X[0][2]+Margin, X[0][3]+Margin), (0, 0, 255), 1);\n",
    "\n",
    "    # 10. Modelling:\n",
    "    Max = 0;\n",
    "    BestI = 0;\n",
    "    BestJ = 0;\n",
    "    LineWeight = 8;\n",
    "    for I, X in enumerate(Lines):\n",
    "        for J, Y in enumerate(Lines):\n",
    "            print(I, len(Lines), end = '\\r');\n",
    "            TmpEdges = copy.copy(Edges);\n",
    "            cv2.line(TmpEdges, (X[0][0]+Margin, X[0][1]+Margin), (X[0][2]+Margin, X[0][3]+Margin), (0, 0, 0), LineWeight);\n",
    "            cv2.line(TmpEdges, (Y[0][0]+Margin, Y[0][1]+Margin), (Y[0][2]+Margin, Y[0][3]+Margin), (0, 0, 0), LineWeight);\n",
    "            Count = np.count_nonzero(TmpEdges == 0);\n",
    "            if (Count > Max):\n",
    "                Max = Count;\n",
    "                BestI = I;\n",
    "                BestJ = J;\n",
    "    print(\"\");\n",
    "\n",
    "    X = Lines[BestI][0];\n",
    "    Y = Lines[BestJ][0];\n",
    "    \n",
    "    #Cartesian coordinates:\n",
    "    A = (X[0]+Margin, X[1]+Margin);\n",
    "    B = (X[2]+Margin, X[3]+Margin);\n",
    "    C = (Y[0]+Margin, Y[1]+Margin);\n",
    "    D = (Y[2]+Margin, Y[3]+Margin);\n",
    "\n",
    "    #Draw results:\n",
    "    cv2.line(DrawnEdges, A, B, (255, 0, 255), 5);\n",
    "    cv2.line(DrawnEdges, C, D, (255, 0, 255), 5);\n",
    "    cv2.circle(Result, A, 5, (0, 255, 255), -1);\n",
    "    cv2.circle(Result, B, 5, (0, 255, 255), -1);\n",
    "    cv2.circle(Result, C, 5, (0, 255, 255), -1);\n",
    "    cv2.circle(Result, D, 5, (0, 255, 255), -1);\n",
    "\n",
    "    A = np.asarray(A);\n",
    "    B = np.asarray(B);\n",
    "    C = np.asarray(C);\n",
    "    D = np.asarray(D);\n",
    "    Center = (A+B+C+D)/4;\n",
    "\n",
    "    #Sort vertex:\n",
    "    Vertex = list();\n",
    "    Vertex.append(Cart2Pol(A-Center));\n",
    "    Vertex.append(Cart2Pol(B-Center));\n",
    "    Vertex.append(Cart2Pol(C-Center));\n",
    "    Vertex.append(Cart2Pol(D-Center));\n",
    "    Vertex.sort(key = lambda X: X[1]);\n",
    "    \n",
    "    #Add results to JSON:\n",
    "    JSON[\"Results\"] = dict();\n",
    "    JSON[\"Results\"][\"L0\"] = (Pol2Cart(Vertex[3])+Center).tolist();\n",
    "    JSON[\"Results\"][\"R0\"] = (Pol2Cart(Vertex[2])+Center).tolist();\n",
    "    JSON[\"Results\"][\"L1\"] = (Pol2Cart(Vertex[0])+Center).tolist();\n",
    "    JSON[\"Results\"][\"R1\"] = (Pol2Cart(Vertex[1])+Center).tolist();\n",
    "    \n",
    "    #Show results:\n",
    "    cv2.imshow(\"RawImage\", RawImage);\n",
    "    cv2.imshow(\"Contrast\", Contrast);\n",
    "    cv2.imshow(\"Blur1\", Blur1);\n",
    "    cv2.imshow(\"Blur2\", Blur2);\n",
    "    cv2.imshow(\"Edges1\", Edges1);\n",
    "    cv2.imshow(\"Edges2\", Edges2);\n",
    "    cv2.imshow(\"Drawn Edges\", DrawnEdges);\n",
    "    cv2.imshow(\"Result\", Result);\n",
    "    \n",
    "    #Export images:\n",
    "    if (ExportPath is not None):\n",
    "        cv2.imwrite(ExportPath+\"/01 Image.jpg\", RawImage);\n",
    "        cv2.imwrite(ExportPath+\"/02 Contrast.jpg\", Contrast);\n",
    "        cv2.imwrite(ExportPath+\"/03 Blur1.jpg\", Blur1);\n",
    "        cv2.imwrite(ExportPath+\"/04 Blur2.jpg\", Blur2);\n",
    "        cv2.imwrite(ExportPath+\"/05 Edges1.jpg\", Edges1);\n",
    "        cv2.imwrite(ExportPath+\"/06 Edges2.jpg\", Edges2);\n",
    "        cv2.imwrite(ExportPath+\"/07 DrawnEdges.jpg\", DrawnEdges);\n",
    "        cv2.imwrite(ExportPath+\"/08 Result.jpg\", Result);\n",
    "\n",
    "    cv2.waitKey(0);\n",
    "    cv2.destroyAllWindows();\n",
    "\n",
    "    return JSON;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main explanation\n",
    "##### is not a definitive one, for now its for you to understand my idea and give me a feedback\n",
    "using this code we have a main that use the python library wrote by us. This main read the image into the Path variable and create a folder named Results inside the folder where is this notebook and after that create a folder with the same name of the image where we can fine all the image processed to achieve the final result.\n",
    "instead of writing a aprameter into the command line, we only need to change the value of the variable  Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 5252\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PIL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88ca6d8ab96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mMain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-88ca6d8ab96f>\u001b[0m in \u001b[0;36mMain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mJSON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSONPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mJSON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdentifyRunway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExportPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mJSON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExportPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-aed15945a011>\u001b[0m in \u001b[0;36mfind_position\u001b[0;34m(JSON, image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#gets image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimageSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PIL' is not defined"
     ]
    }
   ],
   "source": [
    "def ReadJSON(JSONPath, Path):\n",
    "    File = open(JSONPath, \"r\");\n",
    "    Data = json.loads(File.read());\n",
    "    Data[\"Image\"] = Path;\n",
    "    File.close();\n",
    "    return Data;\n",
    "\n",
    "def Main():\n",
    "    Path='../DataSet/Selected/Real/06.jpg'\n",
    "    #Path='../DataSet/Selected/Simulation/07.jpg'\n",
    "    ExportPath = './Results' + '/' + os.path.splitext(os.path.basename(Path))[0]\n",
    "    \n",
    "    if (not os.path.exists('./Results')):\n",
    "        os.makedirs('./Results', exist_ok = True);\n",
    "    \n",
    "    if (not os.path.exists(ExportPath)):\n",
    "        os.makedirs(ExportPath, exist_ok = True);\n",
    "                           \n",
    "    JSONPath = os.path.splitext(Path)[0]+\".txt\";\n",
    "    if (not os.path.exists(Path)):\n",
    "        sys.exit(\"File does not exist.\");\n",
    "    if (not os.path.exists(JSONPath)):\n",
    "        sys.exit(\"JSON file does not exist.\");\n",
    "\n",
    "    JSON = ReadJSON(JSONPath, Path);\n",
    "    JSON = IdentifyRunway(JSON, ExportPath);\n",
    "    JSON = find_position(JSON, Path);\n",
    "    print (JSON)\n",
    "    Display(JSON, ExportPath);\n",
    "    Correct(JSON, ExportPath);\n",
    "    \n",
    "    #Export JSON:\n",
    "    if (ExportPath is not None):\n",
    "        File = open(ExportPath + \"/11 JSON.txt\", \"w\");\n",
    "        json.dump(JSON, File, indent = 4);\n",
    "        File.close();\n",
    "        \n",
    "if (__name__ == \"__main__\"):\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
